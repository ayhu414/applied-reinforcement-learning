{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb46d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from gym.envs.classic_control import CartPoleEnv\n",
    "from rlutils import Agent, enact_policy, evaluate_agent, state_cols\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0d3c1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Actions/State\n",
    "\n",
    "\n",
    "        | (0,0) | U/1|  (2,0) |\n",
    "        |L/4    | X  | R/2    |\n",
    "        | (0,2) | D/3| (2,2)  |\n",
    "\"\"\" \n",
    "\n",
    "\n",
    "class BPFEnv():\n",
    "    def __init__(self, n_residues_max, residues):\n",
    "        assert len(residues) <= n_residues_max, \"Invalid Protein\"\n",
    "        # self.hyperparameters = ? may need some here\n",
    "        residues_ = []\n",
    "        for i in range(len(residues)):\n",
    "            if(i < len(residues)):\n",
    "                residues_ += [residues[i]]\n",
    "            else:\n",
    "                residues_ += [0]\n",
    "        self.state = np.array(([0]*(n_residues_max*2 + 1)**2) + residues_ + [n_residues_max, n_residues_max])\n",
    "        self.n_residues_max = n_residues_max\n",
    "        # flat representation of the state\n",
    "    def make_state_pretty(self):\n",
    "        #convert the flat representation of the state into something more easily useable\n",
    "        nrm = self.n_residues_max\n",
    "        grid = self.state[:(2*nrm+1)**2].reshape(2*nrm+1, 2*nrm+1).T\n",
    "        residues = self.state[(2*nrm+1)**2:(2*nrm+1)**2+nrm]\n",
    "        pos = (self.state[-2], self.state[-1])\n",
    "        return (grid, residues, pos)\n",
    "    def state_from_pretty(self, pretty):\n",
    "        grid = pretty[0]\n",
    "        residues = pretty[1]\n",
    "        pos = pretty[2]\n",
    "        state_ = np.concatenate([grid.flatten(), residues], axis=0)\n",
    "        state_ = np.concatenate([state_, pos], axis=0)\n",
    "        self.state = state_\n",
    "        \n",
    "    def increment(self, action):\n",
    "        # double check the move makes sense\n",
    "        assert action in [1,2,3,4], \"Action \" +str(action) + \" is invalid\"\n",
    "        \n",
    "        state_prime = np.copy(self.state)\n",
    "        board = state_prime[0]\n",
    "        residues = state_prime[1]\n",
    "        position = state_prime[2]\n",
    "        \n",
    "        position[0] += (action % 2 ==0) * (np.sign(3-action)) # i.e. if the action is divisible by 2 (right or left) with appropriate sign\n",
    "        position[1] += (not action%2 == 0) * (np.sign(action-2)) # these lines just move where our cursor is\n",
    "    \n",
    "        board[position] = residues[0] # change board at cursor to leading residues\n",
    "        residues = [residues[i] for i in range(1, len(residues))] # remove first residue\n",
    "        \n",
    "        return [board, residues, new_position]\n",
    "    def reset(self):\n",
    "        return self.state, {}\n",
    "    def compute_energy(self, state):\n",
    "        # first attempt at the energy functional\n",
    "        side_of_board_reward = 0\n",
    "        energy = 0\n",
    "        board = state[0]\n",
    "        for i in range(len(board)):\n",
    "            for j in range(len(board[i])):\n",
    "                ep = 0\n",
    "                if(j < len(board[i])-1):\n",
    "                    ep += np.abs(board[i][j+1]+board[i][j])\n",
    "                else:\n",
    "                    ep += side_of_board_reward\n",
    "                if(j > 0):\n",
    "                    ep += np.abs(board[i][j-1]+board[i][j])\n",
    "                else:\n",
    "                    ep += side_of_board_reward\n",
    "                if(i > 0):\n",
    "                    ep += np.abs(board[i-1][j] + board[i][j])\n",
    "                else:\n",
    "                    ep += side_of_board_reward\n",
    "                if(i < len(board[j]-1)):\n",
    "                    ep += np.abs(board[i+1][j] + board[i][j])\n",
    "                else:\n",
    "                    ep += side_of_board_reward\n",
    "                # if any of the residues match, they are awarded 2 points. If they are blank, they are awarded 1pt\n",
    "                # if they are different they are awarded 0 points. We then\n",
    "                energy += ep\n",
    "                    \n",
    "    def step(self, action):\n",
    "        assert action in [1,2,3,4], \"Action \" +str(action) + \" is invalid\"\n",
    "        \n",
    "        reward = 0\n",
    "        new_state = self.increment(action)\n",
    "        \n",
    "        #now we need to implement the rewards\n",
    "        \n",
    "        #SELF - AVOIDING\n",
    "        if(not np.count_nonzero(new_state[0]) > np.count_nonzero(state[0])):\n",
    "            # this means the action overlapped with a previous residue\n",
    "            reward += 0.01\n",
    "            done = True\n",
    "        if(len(self.state[2]) == 0):\n",
    "            reward += 0.5\n",
    "            done = True #finished protein\n",
    "        #ENERGY \n",
    "        if(not done):\n",
    "            reward += self.compute_energy(new_state)\n",
    "        \n",
    "        \n",
    "        return new_state, reward, done, {}\n",
    "\n",
    "    def show(self):\n",
    "        # display a picture of the current state\n",
    "        state_ = self.make_state_pretty()\n",
    "        x = state_[0]\n",
    "        green = [[i,j] for i in range(len(x)) for j in range(len(x[i])) if x[i][j] == -1]\n",
    "        blue =  [[i,j] for i in range(len(x)) for j in range(len(x[i])) if x[i][j] == 1]\n",
    "        green = np.array(green)\n",
    "        blue = np.array(blue)\n",
    "        plt.scatter(green[:,0],green[:,1],color=\"green\", lw=5)\n",
    "        plt.scatter(blue[:, 0], blue[:, 1],color=\"teal\", lw=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3bc1efe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0 -1  1 -1  1  4  4]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5468/1912318432.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_state_pretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_from_pretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "env = BPFEnv(4, [-1,1,-1,1])\n",
    "print(env.state)\n",
    "sp = env.make_state_pretty()\n",
    "print(env.state_from_pretty(sp).state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a19feab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(Agent):\n",
    "    \"\"\"An agent trained using DQN\"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=0.9, epsilon=0.75, epsilon_decay=0.01, epsilon_min=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gamma: Discount rate for future rewards\n",
    "            epsilon: Exploration value\n",
    "            epsilon_decay: How much we decay the rewards after each update\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Make a model that predicts the value of a move and actions\n",
    "        self.q_function = self.make_q_function()\n",
    "        self.q_function.compile(loss='mse', optimizer='adam')\n",
    "        \n",
    "        # Memory for all observed moves\n",
    "        self.memory = pd.DataFrame()\n",
    "        self.max_memory = 2048\n",
    "        \n",
    "    def make_q_function(self):\n",
    "        \"\"\"Generate a Q-function that computes the value of both actions given state\"\"\"\n",
    "        return Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(22,)),\n",
    "            Dense(64, activation='relu', input_shape=(4,)),\n",
    "            Dense(2, activation='linear')\n",
    "        ])\n",
    "    \n",
    "    def get_action(self, state):       \n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Choose action randomly\n",
    "            return np.random.randint(1,5)\n",
    "        else:\n",
    "            # Compute the value of each move\n",
    "            q_values = self.q_function.predict(state[np.newaxis, :])[0]\n",
    "            \n",
    "            # Pick the best value\n",
    "            return np.argmax(q_values)\n",
    "    \n",
    "    def train(self, states):\n",
    "        # Compute the next state for each state\n",
    "        #  Numpy roll rotates the array from [1, ... N] to [2, ... N, 1]\n",
    "        next_state_cols = []  # Stores the columns in the DataFrame that involve refitting the \n",
    "        for c in state_cols:\n",
    "            next_state_cols.append(f'next_{c}')\n",
    "            states[f'next_{c}'] = np.roll(states[c], -1)\n",
    "        \n",
    "        # Add new states to the memory\n",
    "        self.memory = pd.concat([self.memory, states])\n",
    "        \n",
    "        # If needed, sample fewer points from the memory to keep it from becoming too big\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory = self.memory.sample(self.max_memory)\n",
    "        \n",
    "        # Get compute the Q value for the next state\n",
    "        #  The value is zero for the last point because there is no next state\n",
    "        q_value_next = np.max(self.q_function.predict(self.memory[next_state_cols].values), axis=1)\n",
    "        q_value_next = np.where(self.memory['done'], 0, q_value_next)\n",
    "        \n",
    "        # Compute the target Q-values\n",
    "        q_target = self.memory['reward'].values + self.gamma * q_value_next\n",
    "        \n",
    "        # Save the old weights\n",
    "        self.q_function.fit(self.memory[state_cols].values, q_target, shuffle=True, batch_size=32, verbose=False)\n",
    "        \n",
    "        # Last step, make the algorithm more greedy\n",
    "        self.epsilon *= (1 - self.epsilon_decay)\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        return \n",
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4400ec81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a6b38aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5468/97325573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdqn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/josep/Desktop/Winter 2022/MENGAI/applied-reinforcement-learning/rlutils.py\u001b[0m in \u001b[0;36mevaluate_agent\u001b[0;34m(env, agent, n_episodes, train)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Run the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menact_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/josep/Desktop/Winter 2022/MENGAI/applied-reinforcement-learning/rlutils.py\u001b[0m in \u001b[0;36menact_policy\u001b[0;34m(env, agent)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Step until \"done\" flag is thrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Just push it to one side as an example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5468/3024096225.py\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Compute the value of each move\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mstate_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mstate_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mstate_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mstate_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "dqn_results = evaluate_agent(env, agent, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9fb9d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BPFEnv(4, [-1,1,-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99bdae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
